{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\pytgpu\\Lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Loss: 0.1460\n",
      "Epoch [2000/10000], Loss: 0.0792\n",
      "Epoch [3000/10000], Loss: 0.0343\n",
      "Epoch [4000/10000], Loss: 0.0172\n",
      "Epoch [5000/10000], Loss: 0.0104\n",
      "Epoch [6000/10000], Loss: 0.0071\n",
      "Epoch [7000/10000], Loss: 0.0053\n",
      "Epoch [8000/10000], Loss: 0.0042\n",
      "Epoch [9000/10000], Loss: 0.0034\n",
      "Epoch [10000/10000], Loss: 0.0028\n",
      "Input: [0.0, 0.0] -> Predicted: 0.0035, Target: 0.0\n",
      "Input: [0.0, 1.0] -> Predicted: 0.0477, Target: 0.0\n",
      "Input: [1.0, 0.0] -> Predicted: 0.0478, Target: 0.0\n",
      "Input: [1.0, 1.0] -> Predicted: 0.9176, Target: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# AND Gate data\n",
    "X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y = torch.tensor([[0.0], [0.0], [0.0], [1.0]])\n",
    "\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the layers of the network\n",
    "        self.fc1 = nn.Linear(2, 2)  # 2 input features, 2 neurons in hidden layer\n",
    "        self.fc2 = nn.Linear(2, 1)  # 2 neurons in hidden layer, 1 output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = torch.sigmoid(self.fc1(x))  # Activation function (sigmoid) for hidden layer\n",
    "        x = torch.sigmoid(self.fc2(x))  # Activation function (sigmoid) for output layer\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = MLP()\n",
    "\n",
    "# Loss function (Mean Squared Error for binary classification)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the trained model on AND gate inputs\n",
    "with torch.no_grad():  # No need to compute gradients for testing\n",
    "    for input_data, target in zip(X, y):\n",
    "        output = model(input_data)\n",
    "        print(\n",
    "            f\"Input: {input_data.tolist()} -> Predicted: {output.item():.4f}, Target: {target.item()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
